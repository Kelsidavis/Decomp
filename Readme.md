# üîé Ghidra-LLM Reverse Engineering Pipeline

This project bundles **Ghidra (headless mode)**, a set of custom Python scripts, and a connected **LLM endpoint** (e.g. Qwen3, GPT-style models) into a single Docker workflow for semi-automated reverse engineering.

The pipeline takes a Windows PE binary (`.exe`), decompiles it, generates human-readable stubs, extracts embedded assets, and scaffolds a compilable project (`C` or `C++`) that you can iterate on.

---

## üì¶ Features

* **Headless Ghidra analysis**
  Dumps functions, imports, and PE resources.

* **Function Hunt & Humanization**
  Analyzes functions, filters/dedupes, and calls an LLM to label them.
  Produces `functions.labeled.jsonl`, rewrites source tree with human-readable names, and writes a `report.md`.

* **LLM explanation**
  Calls your configured model endpoint to annotate functions with JSON labels, progress %, elapsed time, and ETA in logs.

* **Code scaffolding**
  Converts the report into compilable stubs (`.c` or `.cpp`).

* **Asset extraction**

  * PE resources (icons, bitmaps).
  * Magic-based carving from binary.
  * Embeds assets directly into C code for buildability.

* **Windows build generator**
  Creates `recovered_project_win/` with CMake files.
  Links system DLLs, generates dynamic wrappers for vendor DLLs, and builds with MinGW.

* **Enhanced logging**
  Both `run.sh` and `humanize.sh` now prefix logs with timestamps, show live progress %, elapsed time, and estimated time remaining. Logs are written to `work/logs/` and `work/run.<timestamp>.log`.

---

## üöÄ Usage

### 1. Build the image

```bash
docker build -t ghidra-llm:latest .
```

### 2. Place your binary

```
work/target.exe
```

### 3. Run the full pipeline

```bash
./run.sh --exe work/target.exe
```

This will:

* Launch the Docker container.
* Run headless Ghidra.
* Extract functions, imports, and resources.
* Call the LLM to label and humanize.
* Generate a compilable project under `work/recovered_project/`.

### 4. Humanize only (optional re-run)

```bash
./humanize.sh --topn 500 --min-size 16
```

---

## ‚öôÔ∏è Environment Variables

* `BINARY_PATH` ‚Äì input binary (`.exe`).
* `OUT_JSON` ‚Äì path to dump functions JSON.
* `REPORT_MD` ‚Äì path for human-readable LLM explanation.
* `CODE_LANG` ‚Äì `c`, `cpp`, or `auto` (default).
* `HUMANIZE_MODE` ‚Äì `off`, `suggest`, or `apply`.
* `BUILD_RECOVERED` ‚Äì `1` to attempt a `make` build of the applied project.
* `LLM_ENDPOINT` ‚Äì model API URL (defaults to localhost:8080).
* `LLM_MODEL` ‚Äì model name (e.g. `Qwen3-14B-UD-Q5_K_XL.gguf`).
* `HUNT_TOPN` ‚Äì number of functions to keep by size.
* `HUNT_LIMIT` ‚Äì hard cap on number of functions.
* `HUNT_MIN_SIZE` ‚Äì drop functions below N bytes.
* `HUNT_CAPA`, `HUNT_YARA` ‚Äì enable/disable enrichment.
* `HUNT_LLM_CONCURRENCY` ‚Äì number of concurrent LLM requests.
* `HUNT_LLM_MAX_TOKENS` ‚Äì max tokens per function label (increase for more detail, at cost of speed).

---

## üìÇ Output Layout

* `recovered_project/`

  * `include/` ‚Äì headers (`recovered.h`, `resources.h`)
  * `src/` ‚Äì function stubs + `resources_embedded.c`
  * `assets/` ‚Äì extracted icons, bitmaps, carved files
  * `report.md` ‚Äì annotated function descriptions

* `work/hunt/`

  * `functions.enriched.jsonl` ‚Äì enriched with capa/yara.
  * `functions.labeled.jsonl` ‚Äì LLM labels (names, tags, inputs/outputs).
  * `report.md` ‚Äì human-readable explanations.

* `recovered_project_human/`
  Humanized project (if enabled).

* `recovered_project_win/`
  Windows build scaffold with CMake + vendor shims.

* `run.<timestamp>.log` and `work/logs/pipeline.<timestamp>.log`
  Full logs with timestamps, % complete, and ETA.

---

## üõ† Building for Windows

```bash
cd work/recovered_project_win/build
cmake ..
make
```

---

## ‚ö†Ô∏è Notes

* Many stubs will be placeholders until you refine them.
* Assets may be incomplete if the binary didn‚Äôt contain resources.
* Vendor DLLs (FMOD, DivX, etc.) are stubbed dynamically; you‚Äôll need the SDK for full functionality.
* Expect to iterate: the goal is a **compilable baseline** close to original source, not a 1:1 decompile.

---

## üß° Next Steps

* Replace vendor stubs with real SDK headers/libs.
* Use the LLM-generated docs (`report.md`) and `functions.labeled.jsonl` to re-implement function logic.
* Refactor and test: compile ‚Üí run under Wine/Windows ‚Üí fix.

